\chapter*{Appendix III: Literature studies}

\section{Synthesizing Board Evaluation Functions for Connect4 using Machine Learning Techniques} 
\cite{stenmark:masters}
This work contains a description of minimax and the alpha-beta enhancement.  An evaluation function IBEF is defined as an initial attempt.  This function place a weight on each square.  The weight is simply the count of the number of lines that crosses through the square: for example left bottom corner has value 3.  IBEF is the sum of a positive weight of squares occupied by the player minus the sum of the negative weights of the squares occupied by the opponent. Experiments show that the first player wins a random player at about $90\%$ and the second player wins about $98\%$ of the time (one level search). 
\subsection{The ADATE Experiment}
\label{sec:ADATE}
The ADATE system does automatic programming using standard ML (Olsson,1995).  The aim was to use ADATE to come up with a function that beats IBEF.  For this a set of example start positions were provided -- the number of wins for this example set provides the fitness function for ADATE. The examples given were the starting position, all possible first moves and all possible second moves (a total of 57 examples).  Then a number of examples positions were added by picking them from random playing games (the details of how selection was done is not described).  Two sets of examples are provided -- the training set of 155, and the validation set of 556.  Both sets contain the 57 fixed examples, but differ in their random examples (I assumed).  The training process used level and level 2 sets -- switching the first player.

The function produced by ADATE is a linear function with weights associated with each square.  Essentially the function has the same structure as IBEF, but with different values. When compared to IBEF, the ADATE function as first player beats IBEF at minimax level 1,2,3 and 5.  As second player ADATE beats IBEF on all levels.  This indicates that being the first player has an advantage.

\subsection{Reinforcement learning experiment}  Instead of using a linear function, an artificial neural network (ANN) was used. The ANN takes 126 input values - three values per square. Each value is binary, only one of the values can be 1. The first value is 1 if occupied by the first player, second value is 1 if it is occupied by the second player, else the third value is 1.  The output of the network is the evaluation function of the position. One hidden layer consisting of 15 neurons was used and training was against IBEF.  The resulting function is called the RL function.  The first player RL beats IBEF on levels 1,3,4 and 5.  As the second player, RL beats IBEF only at level 1 and 2.  

\subsection{ADATE vs. RL}
The ADATE function was played against the RL function using various levels. For instance, first player RL at level 1 and 2 beats ADATE from level 1 to 6. But, when RL plays at level 3,5 and 6, the ADATE function dominates.  In fact, first player RL is particularly weak at level 3. As second player, RL  is more consistently dominant. Interestingly when both are played against IBEF, the ADATE function fares better than the RL function.   

\subsection{References} Was cited by \cite{konen:failures} (uses tictactoe statistics), \cite{konen:games} (in relation to the tictactoe work), \cite{thill:connect-4} (concludes that TDL performed slightly better than automatic programming) ,\cite{diez:minimax} (uses the IBEF evaluation function heuristic for measuring their experiments).

\subsection{Notes} I like the simplicity of the IBEF heuristic -- and other work done here provides interesting statistics for comparisons to do related work.    

It is not clear that ADATE is the best way to optimize the given linear structure -- I think it is a bad example for this type of learning.  It could be that other optimization technologies (such as PSO) may fare better here.

The encoding for RL is very complex -- the linear function seems to capture much more domain knowledge.  It could be by simply coding the ANN differently it may be easier to learn the function.

More cohesive results could have been obtained by choosing functions that are more unifiable.  The RL function is not linear and as such when comparing it to a linear function does not prove that RL is better than ADATE -- it could mean that non-linear is better than linear.  A better approach might have been to learn the same function (i.e. the IBEF structure) using RL methods.  This would have meant not using a neural network.

There is an interesting problem highlighted with the comparison of the evaluation function using minimax.  I had the same problem in \refsec{tree-exp-bfss-ab}.  I could not really decide which algorithm was best.  It may be interesting to see whether an approach to playing against different levels could produce a clearer result.  Or perhaps applying my algorithm using these functions as examples. 

\section{Rminimax: An Optimally Randomized MINIMAX Algorithm}
\cite{diez:minimax}
\subsection{Introduction} Minimax is completely deterministic and therefor can become annoying for the human opponent.  The idea is to randomize the strategy while remaining optimal.  The idea is based on a concept called bounded rationality -- and it can be regarded as the application of a randomized shortest-path framework.  Basically, the Rminimax algortihm (i) models non-rational players, (ii) controls player strength and (iii) avoids predictability. 
\subsection{Randomised shortest path (RSP)}  All paths between vertices are enumerated.  Low cost paths are assigned a higher probability. Using the Shannon entropy measure, an equation for the probability of a path in terms of its cost is derived.  This equation follows a Boltzmann distribution. The author assumes that the graph is acyclic; and ends up with recurrence equations.
\subsection{The algorithm} The recurrence equations are used as well as a standard equation for the logarithm of a sum to compute transition probabilities. The input is a game-tree where transitions are tagged with cost.  There is also input that affects the randomisation; a high value for $\theta$ means near-optimal strategy, while a low value is likely to mean a poor strategy. 
\subsection{Simulation results}  The experiment uses a 5-ply look-ahead and alpha-beta pruning to generate the game tree.  Costs are allocated such that winning positions have a higher costs and losing positions a lower cost. The cost of non-edge nodes is $1$.  Because the entire tic-tac-toe game tree can be used, it is a clean way to test the effect of $\theta$.  For connect-four the evaluation function proposed by Stenmark \cite{stenmark:masters} is used.  Both experiments show the expected result: that a higher value for $\theta$ produce a stronger player.  However, the results for connect-4 is not as clear as the result for tic-tac-toe.
\subsection{Notes} This work does not directly relate to the work I am doing -- the focus is on improving the satisfaction of a human player; and a way to adjust the strength of a player.  However, I do have the problem of emulating weak players when comparing different evaluation functions.  It could be that one could express an evaluation function in terms of another using $\theta$.  It has the potential of becoming a knowledge-strength test. 

\section{Symbolic and Neural Learning Algorithms: An Experimental Comparison}
\cite{shavlik:comparision}
\subsection{Introduction}
Supervised learning is implemented using using ID3, perceptron and backpropagation algorithms.  These algorithms are compared by observing the effect of the amount of training data and the quality of training data.  For the latter, random noise was used, values was removed from training set, and features was removed.
% i ignored the work done using the NETTalk dataset; it is not relevant to my current research
\subsection{General experimental setup}
Five different data sets were used: soyabean (17 classes), chess (2 classes), audiology (24 classes), heart disease (2 catagories), NETTalk (41 catagories). 
\subsubsection{ID3} 
For ID3 chi-squared pruning \cite{quinlan:noise} was added to handle noisy data.
\subsubsection{Perceptron}
The perceptron \cite{rosenblatt:perceptrons} adjust the weight of a single linear threshold, and minimizes the squares of errors using gradient decent.  It only works on data that is linearly separable.  The implementation makes use of the perceptron cycling theorem.  One perceptron is created for each category; the perceptron that exceeds his threshold the furthest is chosen.
\subsubsection{Backpropagation} 
The backpropagation \cite{rumelhart:propagation} method also uses gradient descent with sum of squares - but it has hidden units.  This paper uses a differentiable threshold function.  It also employs a learning rate and a momentum term that helps overcome local maxima. This method is not guaranteed to converge.  The number of hidden units were chosen by a formula: (input nodes $+$ output nodes) $\times \frac{10}{100}$
\subsection{Representing examples}
Bit vectors are used: one for each value-attribute pair. Missing values for an attribute means all bits are off.
This representation slightly improved ID3's performance. Possible reasons are: 1) It eliminates the undesired preference for many valued attributes 2) Allows more general branching  and 3) it eliminates the irrelevant values problem.  The downside is that more features means increased run time.
\subsection{Results}
\subsubsection{Learning time and correctness} 
ID3 and perceptron is much faster than BP, but BP did fare better on correctness.
\subsubsection{Effect of the number of training examples}
BP did well here too.  The author suggested that faring better on few examples is a prediction of faring better on many examples.  ID3's lack of performance could be related to the problem of small disjuncts.
\subsubsection{Effect of imperfect examples}
BP did better here, but ID3 did surprisingly well.
\subsection{Notes}    
This paper highlights opportunities to improve on the basic ID3 - specifically the use of bit vectors sounds applicable to the current work.  It is also worth looking at the different types of data sets; asking which type are closer to game boards.  The experiments are likely to be worth repeating; but also including perceptron trees\cite{utgoff:trees}. 
\section{Reinforcement learning with n-tuples on the game Connect-4}
\cite{thill:connect-4}
\subsection{Introduction}
\subsubsection{Learning}
The credit assignment problem is addressed by reinforcement learning methods like temporal difference learning (TDL) and
evolutionary algorithms like evolution strategies and co-evolution.  Games present a challenge because the objective function is not known. Instead of finding this objective function directly, often certain interactions are used to drive the search, thus we end up with a surrogate objective function \cite{krawiec:tuple}.  A well known example of this type of interaction is self-play. 
\subsubsection{Related work}
There has been previous work in the area of learning to play Connect-4.  However, this is the first time a comparison is done with a perfect playing minimax agent. Lucas showed that Othello can be learned with the n-tuple approach \cite{lucas:tuple}.	
\subsection{Methods}
\subsubsection{Connect-4}
The minimax agent used an 8-ply or 12-ply opening database - it finds the perfect next move within a fraction of a second.
\subsubsection{N-tuples and LUTs}
Each n-tuple is a sequence of board locations.  Each location has a value that identifies one in a set of possible states.  Each state is an encoding of some semantic meaning: $0$ is empty, $1$ is yellow, $2$ is red, $3$ is empty and not reachable.  For the experiments, two encoding schemes are compared: $P_4$ and $P_3$.  The former includes state $3$ and the latter excludes it. 

The number represented by the sequence can be used as an index to a lookup table (LUT) which contain a weight. (WD: Thus for an n-tuple we have potentially $n^p$ LUT entries; where p is the number of different states). 

The key, $k$ for the lookup table for an n-tuple, $T_i$ with positions denoted by $\{a_{i0},\ldots,a_{in}\}$ is calculated using the number of states $p$ and the current board position, $z$.
\[
 k_i(z) = \sum_{j=0}^{n-1} z[a_{ij}] p^j 
\]
Where $z[a]$ is the state of the position $z$ at location $a$.

The output function for the n-tuple network with $m$ tuples $\{T_1,\ldots,T_m\}$ is calculated using the lookup table for each tuple, $L_i$. 
\[
f(\vec{w_t},z_t) = \sum_{i=0}^{m} L_i[k_i(z_t)]
\]
Where $L_i[k]$ is the sum of the weights in the lookup table for $T_i$ for key $k$.

Connect-4 has a symmetry along the 	middle column: let key $k$ have a symmetry value of $M(k)$, then we can augment the function to take advantage of the symmetry:
\[
f(\vec{w_t},z_t) = \sum_{i=0}^{m} L_i[k_i(z_t)] + L_i[M(k_i(z_t))]
\] 

N-tuples are created using a method similar to the snake method \cite{lucas:tuple}: Start are a random locations; add one of the $8$ neighbours and walk to it.  Add another neighbour until $n$ locations are added (no duplicates are allowed). 
\subsubsection{TDL}
The goal of the learning agent is to predict the value function: $1$ if yellow wins and $-1$ if red wins.  It does so by playing a sequence of games against itself.  The reward given at the end of the game is the outcome and it corresponds to the value of the value function.
We use \refeq{general td} and $\tanh(f(\vec{w_t},z_t))$ as the function to be learnt.
(Q: what is the derivative of $\tanh$? A: $1 - \tan^2(x)$) % http://math2.org/math/derivatives/more/hyperbolics.htm
\subsubsection{Agent evaluation}
The perfect minimax player (playing red -- second player) is the referee. A sequence of $50$ games are played - points are awarded as $1$ for win and $\frac{1}{2}$ for draw and $0$ for loose. If minimax has more than one equal option, it chooses at random which one to play.  This introduces some randomness during play.  The overall success rate is the mean of the $50$ scores; ranging for $0$ to $1$. 
\subsection{Experimental setup}
During training no search is used.  Used $70$ n-tuples of length $8$.  The learning rate, $\theta$ follows an exponential decay scheme; ranging from $0.01$ to $0.001$.  Initial weights were all zero; and playing $10$ million games, evaluating every $100,000$.
An additional source of randomness is introduced: with some probability $\epsilon$, the learning agent chooses its move on random. After a random move n weight update is performed. A sigmoidal decay scheme is used for $\epsilon$. 
\subsection{Results}
When using two separate LUTs - one for each player an value above $0.8$ was obtained. Using the $P_4$ encoding a faster  learning rate than $P_3$ is obtained.  The specific n-tuples significantly affected the training set - one specific set was found that was unable to be trained at all.  The choice for $\theta$ decay also had a significant impact on learning.
\subsection{Conclusion}
The n-tuple approach is successful for the game of Connect-4; but millions of training examples are required.  Future work can be done on identifying the characteristics of good n-tuple sets.	

\subsection{Notes}
It might be worth trying the perfect player as part of the learning process.  
It is possible to encode TDL so that we can learn the weights of the locations, instead of the weights for the function? Is the symmetry used for Connect-4 a valid one?  The n-tuple seems to use sequences -- why not sets?
The LUTs have a weight for each one of the n-tuples.  What will happen if it has only one weight for the n-tuple?
%thill_2012.pdf

\section{Online Adaptable Learning Rates for the Game Connect-4}
\cite{bagheri:online}
TODO: Read
%bagheri_2014.pdf