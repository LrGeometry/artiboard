\chapter{Literature studies}

\section{Synthesizing Board Evaluation Functions for Connect4 using Machine Learning Techniques} 
\cite{stenmark:masters}
This work contains a description of minimax and the alpha-beta enhancement.  An evaluation function IBEF is defined as an initial attempt.  This function place a weight on each square.  The weight is simply the count of the number of lines that crosses through the square: for example left bottom corner has value 3.  IBEF is the sum of a positive weight of squares occupied by the player minus the sum of the negative weights of the squares occupied by the opponent. Experiments show that the first player wins a random player at about $90\%$ and the second player wins about $98\%$ of the time (one level search). 
\subsection{The ADATE Experiment}
\label{sec:ADATE}
The ADATE system does automatic programming using standard ML (Olsson,1995).  The aim was to use ADATE to come up with a function that beats IBEF.  For this a set of example start positions were provided -- the number of wins for this example set provides the fitness function for ADATE. The examples given were the starting position, all possible first moves and all possible second moves (a total of 57 examples).  Then a number of examples positions were added by picking them from random playing games (the details of how selection was done is not described).  Two sets of examples are provided -- the training set of 155, and the validation set of 556.  Both sets contain the 57 fixed examples, but differ in their random examples (I assumed).  The training process used level and level 2 sets -- switching the first player.

The function produced by ADATE is a linear function with weights associated with each square.  Essentially the function has the same structure as IBEF, but with different values. When compared to IBEF, the ADATE function as first player beats IBEF at minimax level 1,2,3 and 5.  As second player ADATE beats IBEF on all levels.  This indicates that being the first player has an advantage.

\subsection{Reinforcement learning experiment}  Instead of using a linear function, an artificial neural network (ANN) was used. The ANN takes 126 input values - three values per square. Each value is binary, only one of the values can be 1. The first value is 1 if occupied by the first player, second value is 1 if it is occupied by the second player, else the third value is 1.  The output of the network is the evaluation function of the position. One hidden layer consisting of 15 neurons was used and training was against IBEF.  The resulting function is called the RL function.  The first player RL beats IBEF on levels 1,3,4 and 5.  As the second player, RL beats IBEF only at level 1 and 2.  

\subsection{ADATE vs. RL}
The ADATE function was played against the RL function using various levels. For instance, first player RL at level 1 and 2 beats ADATE from level 1 to 6. But, when RL plays at level 3,5 and 6, the ADATE function dominates.  In fact, first player RL is particularly weak at level 3. As second player, RL  is more consistently dominant. Interestingly when both are played against IBEF, the ADATE function fares better than the RL function.   

\subsection{References} Was cited by \cite{konen:failures} (uses tictactoe statistics), \cite{konen:games} (in relation to the tictactoe work), \cite{thill:connect-4} (concludes that TDL performed slightly better than automatic programming) ,\cite{diez:minimax} (uses the IBEF evaluation function heuristic for measuring their experiments).

\subsection{Notes} I like the simplicity of the IBEF heuristic -- and other work done here provides interesting statistics for comparisons to do related work.    

It is not clear that ADATE is the best way to optimize the given linear structure -- I think it is a bad example for this type of learning.  It could be that other optimization technologies (such as PSO) may fare better here.

The encoding for RL is very complex -- the linear function seems to capture much more domain knowledge.  It could be by simply coding the ANN differently it may be easier to learn the function.

More cohesive results could have been obtained by choosing functions that are more unifiable.  The RL function is not linear and as such when comparing it to a linear function does not prove that RL is better than ADATE -- it could mean that non-linear is better than linear.  A better approach might have been to learn the same function (i.e. the IBEF structure) using RL methods.  This would have meant not using a neural network.

There is an interesting problem highlighted with the comparison of the evaluation function using minimax.  I had the same problem in \refsec{tree-exp-bfss-ab}.  I could not really decide which algorithm was best.  It may be interesting to see whether an approach to playing against different levels could produce a clearer result.  Or perhaps applying my algorithm using these functions as examples. 

\section{Rminimax: An Optimally Randomized MINIMAX Algorithm}
\cite{diez:minimax}
\subsection{Introduction} Minimax is completely deterministic and therefor can become annoying for the human opponent.  The idea is to randomize the strategy while remaining optimal.  The idea is based on a concept called bounded rationality -- and it can be regarded as the application of a randomized shortest-path framework.  Basically, the Rminimax algortihm (i) models non-rational players, (ii) controls player strength and (iii) avoids predictability. 
\subsection{Randomised shortest path (RSP)}  All paths between vertices are enumerated.  Low cost paths are assigned a higher probability. Using the Shannon entropy measure, an equation for the probability of a path in terms of its cost is derived.  This equation follows a Boltzmann distribution. The author assumes that the graph is acyclic; and ends up with recurrence equations.
\subsection{The algorithm} The recurrence equations are used as well as a standard equation for the logarithm of a sum to compute transition probabilities. The input is a game-tree where transitions are tagged with cost.  There is also input that affects the randomisation; a high value for $\theta$ means near-optimal strategy, while a low value is likely to mean a poor strategy. 
\subsection{Simulation results}  The experiment uses a 5-ply look-ahead and alpha-beta pruning to generate the game tree.  Costs are allocated such that winning positions have a higher costs and losing positions a lower cost. The cost of non-edge nodes is $1$.  Because the entire tic-tac-toe game tree can be used, it is a clean way to test the effect of $\theta$.  For connect-four the evaluation function proposed by Stenmark \cite{stenmark:masters} is used.  Both experiments show the expected result: that a higher value for $\theta$ produce a stronger player.  However, the results for connect-4 is not as clear as the result for tic-tac-toe.
\subsection{Notes} This work does not directly relate to the work I am doing -- the focus is on improving the satisfaction of a human player; and a way to adjust the strength of a player.  However, I do have the problem of emulating weak players when comparing different evaluation functions.  It could be that one could express an evaluation function in terms of another using $\theta$.  It has the potential of becoming a knowledge-strength test. 

\section{Symbolic and Neural Learning Algorithms: An Experimental Comparison}
\cite{shavlik:comparision}
\subsection{Introduction}
Supervised learning is implemented using using ID3, perceptron and backpropagation algorithms.  These algorithms are compared by observing the effect of the amount of training data and the quality of training data.  For the latter, random noise was used, values was removed from training set, and features was removed.
% i ignored the work done using the NETTalk dataset; it is not relevant to my current research
\subsection{General experimental setup}
Five different data sets were used: soyabean (17 classes), chess (2 classes), audiology (24 classes), heart disease (2 catagories), NETTalk (41 catagories). 
\subsubsection{ID3} 
For ID3 chi-squared pruning \cite{quinlan:noise} was added to handle noisy data.
\subsubsection{Perceptron}
The perceptron \cite{rosenblatt:perceptrons} adjust the weight of a single linear threshold, and minimizes the squares of errors using gradient decent.  It only works on data that is linearly separable.  The implementation makes use of the perceptron cycling theorem.  One perceptron is created for each category; the perceptron that exceeds his threshold the furthest is chosen.
\subsubsection{Backpropagation} 
The backpropagation \cite{rumelhart:propagation} method also uses gradient descent with sum of squares - but it has hidden units.  This paper uses a differentiable threshold function.  It also employs a learning rate and a momentum term that helps overcome local maxima. This method is not guaranteed to converge.  The number of hidden units were chosen by a formula: (input nodes $+$ output nodes) $\times \frac{10}{100}$
\subsection{Representing examples}
Bit vectors are used: one for each value-attribute pair. Missing values for an attribute means all bits are off.
This representation slightly improved ID3's performance. Possible reasons are: 1) It eliminates the undesired preference for many valued attributes 2) Allows more general branching  and 3) it eliminates the irrelevant values problem.  The downside is that more features means increased run time.
\subsection{Results}
\subsubsection{Learning time and correctness} 
ID3 and perceptron is much faster than BP, but BP did fare better on correctness.
\subsubsection{Effect of the number of training examples}
BP did well here too.  The author suggested that faring better on few examples is a prediction of faring better on many examples.  ID3's lack of performance could be related to the problem of small disjuncts.
\subsubsection{Effect of imperfect examples}
BP did better here, but ID3 did surprisingly well.
\subsection{Notes}    
This paper highlights opportunities to improve on the basic ID3 - specifically the use of bit vectors sounds applicable to the current work.  It is also worth looking at the different types of data sets; asking which type are closer to game boards.  The experiments are likely to be worth repeating; but also including perceptron trees\cite{utgoff:trees}. 